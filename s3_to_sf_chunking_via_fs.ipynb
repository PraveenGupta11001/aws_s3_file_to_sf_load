{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pip install snowflake-snowpark-python boto3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import io\n",
    "import os\n",
    "import csv\n",
    "import datetime\n",
    "from snowflake.snowpark import Session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_access_key = 'XXXXXXXXXXXXXXXX'\n",
    "aws_secret_key = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "aws_s3_bucket_name = 'praveeng-s3'  \n",
    "aws_s3_object = 'chunked_csv/'\n",
    "s3 = boto3.client('s3', aws_access_key_id=aws_access_key, aws_secret_access_key=aws_secret_key)\n",
    "chunked_csv_folder = r\"C:\\Users\\DELL\\Desktop\\aws_to_sf\\s3_chunked_csv\"\n",
    "\n",
    "connection_parameters = {\n",
    "   \"account\": \"dr31778.ca-central-1.aws\",\n",
    "   \"user\": \"PRAVEEN11001\",\n",
    "   \"password\": \"XXXXXXXXXXXXXXXX\",\n",
    "   \"role\": \"ACCOUNTADMIN\",  # optional\n",
    "   \"warehouse\": \"COMPUTE_WH\",  # optional\n",
    "   \"database\": \"DEMO\",  # optional\n",
    "   \"schema\": \"s3\"  # optional\n",
    "}\n",
    "\n",
    "session = Session.builder.configs(connection_parameters).create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_chunked_files():\n",
    "    try:\n",
    "        for file in os.listdir(chunked_csv_folder):\n",
    "            file_path = os.path.join(chunked_csv_folder, file)\n",
    "            os.remove(file_path)\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_into_fs_stage(file_to_load):\n",
    "    put_result = session.sql(f\"PUT file://{file_to_load} @fs_stage;\").collect()\n",
    "    print(put_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_into_sf(session, filename):\n",
    "    copy_date = datetime.datetime.utcnow()\n",
    "    \n",
    "    copy_result = session.sql(f\"\"\"\n",
    "    COPY INTO AWS_STUDENT_DATASET_FS(filename, ID, NAME, NATIONALITY, CITY, \n",
    "                              LATITUDE, LONGITUDE, GENDER, ethinic_group, \n",
    "                              AGE, ENGLISH_GRADE, MATH_GRADE, SCIENCE_GRADE, \n",
    "                              LANGUAGE_GRADE, PORTFOLIO_RATING, COVERLETTER_RATING, \n",
    "                              REFLETTER_RATING, copy_date)\n",
    "    FROM \n",
    "        (SELECT '{filename}' AS filename, \n",
    "            $1 AS ID, \n",
    "            $2 AS NAME, \n",
    "            $3 AS NATIONALITY, \n",
    "            $4 AS CITY, \n",
    "            $5 AS LATITUDE, \n",
    "            $6 AS LONGITUDE,\n",
    "            $7 AS GENDER, \n",
    "            $8 AS ethinic_group, \n",
    "            $9 AS AGE, \n",
    "            $10 AS ENGLISH_GRADE, \n",
    "            $11 AS MATH_GRADE, \n",
    "            $12 AS SCIENCE_GRADE, \n",
    "            $13 AS LANGUAGE_GRADE, \n",
    "            $14 AS PORTFOLIO_RATING, \n",
    "            $15 AS COVERLETTER_RATING, \n",
    "            $16 AS REFLETTER_RATING,\n",
    "            '{copy_date}' AS copy_date \n",
    "        FROM @fs_stage/{filename})\n",
    "        \n",
    "    ON_ERROR='SKIP_FILE'\n",
    "    FILE_FORMAT= (FORMAT_NAME=CSV_FORMAT);\n",
    "    \"\"\").collect()\n",
    "\n",
    "    print(copy_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_csv_sf(local_csv_directory):\n",
    "    for file_name in os.listdir(local_csv_directory):\n",
    "        if file_name.endswith('.csv'):\n",
    "            full_path = os.path.join(local_csv_directory, file_name)\n",
    "\n",
    "            load_into_fs_stage(full_path)\n",
    "            copy_into_sf(session, file_name)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = s3.list_objects_v2(Bucket=aws_s3_bucket_name, Prefix='xmls')\n",
    "\n",
    "for obj in response['Contents']:\n",
    "    key = obj['Key']\n",
    "    if key.endswith('set.csv'):\n",
    "        obj_size = obj['Size']\n",
    "        response = s3.get_object(Bucket=aws_s3_bucket_name, Key=key)\n",
    "        file_content = response['Body'].read().decode('utf-8')\n",
    "\n",
    "        if obj_size > 10 * 1024:\n",
    "            chunk_size = 10 * 1024  \n",
    "            reader = csv.reader(io.StringIO(file_content))\n",
    "            headers = next(reader)  \n",
    "            chunks = []\n",
    "            current_chunk = [','.join(headers)]  # Include header in the first chunk\n",
    "            current_chunk_size = len(','.join(headers).encode('utf-8'))\n",
    "\n",
    "            for row in reader:\n",
    "                row_str = ','.join(row)\n",
    "                row_size = len(row_str.encode('utf-8'))\n",
    "                if current_chunk_size + row_size <= chunk_size:\n",
    "                    current_chunk.append(row_str)\n",
    "                    current_chunk_size += row_size\n",
    "                else:\n",
    "                    chunks.append(current_chunk)\n",
    "                    current_chunk = [','.join(headers), row_str]  \n",
    "                    current_chunk_size = len(','.join(headers).encode('utf-8')) + row_size\n",
    "\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                chunked_filename = f\"{chunked_csv_folder}\\{key.split('/')[1].split('.')[0]}_{i}.csv\".replace('-', '_')\n",
    "                partition_data = '\\n'.join(chunk)\n",
    "                with open(chunked_filename, 'w', encoding='utf-8') as file:\n",
    "                    file.write(partition_data)\n",
    "                print(f\"saved partition {i + 1} of {key} to local file system: {chunked_filename}\")\n",
    "        else:\n",
    "            print(f\"saved partition {i + 1} of {key} to local file system: {chunked_filename}\")\n",
    "            chunked_filename = f\"{chunked_csv_folder}{key.split('/')[1]}\".replace('-', '_')\n",
    "            with open(chunked_filename, 'w', encoding='utf-8') as file:\n",
    "                file.write(partition_data, )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_csv_sf(chunked_csv_folder)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_chunked_files()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
