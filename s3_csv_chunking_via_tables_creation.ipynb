{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import io\n",
    "import csv\n",
    "import snowflake.connector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_access_key = 'XXXXXXXXXXXXXXXX'\n",
    "aws_secret_key = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "aws_s3_bucket_name = 'XXXXXXXXXXXXXXXX'  \n",
    "aws_s3_object = 'chunked_csv/'\n",
    "\n",
    "snowflake_account = 'kd66798.ca-central-1.aws'\n",
    "snowflake_user = 'PRAVEEN11001'\n",
    "snowflake_password = 'XXXXXXXXXXXXXXXX'\n",
    "snowflake_database = 'DEMO'\n",
    "snowflake_schema = 'S3_OBJECTS'\n",
    "\n",
    "conn = snowflake.connector.connect(\n",
    "    user=snowflake_user,\n",
    "    password=snowflake_password,\n",
    "    account=snowflake_account,\n",
    "    database=snowflake_database,\n",
    "    schema=snowflake_schema\n",
    ")\n",
    "\n",
    "cur = conn.cursor()\n",
    "s3 = boto3.client('s3', aws_access_key_id=aws_access_key, aws_secret_access_key=aws_secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded chunked_csv/aws_11001_credentials.csv to S3.\n",
      "Uploaded chunked_csv/my_data.csv to S3.\n",
      "Uploaded partition 1 of xmls/student-dataset.csv to S3 with key: chunked_csv/student_dataset_0.csv\n",
      "Uploaded partition 2 of xmls/student-dataset.csv to S3 with key: chunked_csv/student_dataset_1.csv\n",
      "Uploaded partition 3 of xmls/student-dataset.csv to S3 with key: chunked_csv/student_dataset_2.csv\n"
     ]
    }
   ],
   "source": [
    "response = s3.list_objects_v2(Bucket=aws_s3_bucket_name, Prefix='xmls')\n",
    "\n",
    "for obj in response['Contents']:\n",
    "    key = obj['Key']\n",
    "    if key.endswith('.csv'):\n",
    "        obj_size = obj['Size']\n",
    "        response = s3.get_object(Bucket=aws_s3_bucket_name, Key=key)\n",
    "        file_content = response['Body'].read().decode('utf-8')\n",
    "\n",
    "        if obj_size > 10 * 1024:\n",
    "            chunk_size = 10 * 1024  \n",
    "            reader = csv.reader(io.StringIO(file_content))\n",
    "            headers = next(reader)  \n",
    "            chunks = []\n",
    "            current_chunk = [','.join(headers)]  # Include header in the first chunk\n",
    "            current_chunk_size = len(','.join(headers).encode('utf-8'))\n",
    "\n",
    "            for row in reader:\n",
    "                row_str = ','.join(row)\n",
    "                row_size = len(row_str.encode('utf-8'))\n",
    "                if current_chunk_size + row_size <= chunk_size:\n",
    "                    current_chunk.append(row_str)\n",
    "                    current_chunk_size += row_size\n",
    "                else:\n",
    "                    chunks.append(current_chunk)\n",
    "                    current_chunk = [','.join(headers), row_str]  \n",
    "                    current_chunk_size = len(','.join(headers).encode('utf-8')) + row_size\n",
    "\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                partition_key = f\"{aws_s3_object}{key.split('/')[1].split('.')[0]}_{i}.csv\".replace('-', '_')\n",
    "                partition_data = '\\n'.join(chunk)\n",
    "                s3.put_object(Bucket=aws_s3_bucket_name, Key=partition_key, Body=partition_data.encode('utf-8'))\n",
    "                print(f\"Uploaded partition {i + 1} of {key} to S3 with key: {partition_key}\")\n",
    "        else:\n",
    "            s3.put_object(Bucket=aws_s3_bucket_name, Key=f\"{aws_s3_object}{key.split('/')[1]}\".replace('-', '_'), Body=file_content.encode('utf-8'))\n",
    "            print(f\"Uploaded {aws_s3_object}{key.split('/')[1].replace('-', '_')} to S3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_s3_objects(bucket_name, prefix):\n",
    "    try:\n",
    "        response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "        if 'Contents' in response:\n",
    "            for obj in response['Contents']:\n",
    "                s3.delete_object(Bucket=bucket_name, Key=obj['Key'])\n",
    "                print(f\"Deleted object: {obj['Key']}\")\n",
    "        else:\n",
    "            print(f\"No objects found with prefix: {prefix}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "response = s3.list_objects_v2(Bucket=aws_s3_bucket_name, Prefix=aws_s3_object)\n",
    "\n",
    "for obj in response['Contents']:\n",
    "    key = obj['Key']\n",
    "    if key.endswith('.csv'):\n",
    "        table_name = key.split('/')[-1].split('.')[0]  \n",
    "        print(f\"Creating table for {table_name}\")\n",
    "        create_table_query = f\"CREATE OR REPLACE TABLE {table_name} (\"\n",
    "\n",
    "        obj = s3.get_object(Bucket=aws_s3_bucket_name, Key=key)\n",
    "        csv_content = obj['Body'].read().decode('utf-8').split('\\n')\n",
    "        headers = csv_content[0].split(',')\n",
    "\n",
    "        for header in headers:\n",
    "            create_table_query += f'\"{header.strip()}\" VARCHAR,'\n",
    "\n",
    "        create_table_query = create_table_query.rstrip(',') + \")\"\n",
    "        print(create_table_query)\n",
    "        cur.execute(create_table_query)\n",
    "\n",
    "        copy_query = f\"COPY INTO {table_name} FROM \"\n",
    "        copy_query += f\"s3://{aws_s3_bucket_name}/{key} \"\n",
    "        copy_query += \"CREDENTIALS=(AWS_KEY_ID='{0}' AWS_SECRET_KEY='{1}') \".format(aws_access_key, aws_secret_key)\n",
    "        copy_query += \"FILE_FORMAT=(TYPE=CSV FIELD_OPTIONALLY_ENCLOSED_BY='\\\"' SKIP_HEADER=1)\"\n",
    "        print(f\"Copying data into {table_name}\")\n",
    "        cur.execute(copy_query)\n",
    "\n",
    "cur.close()\n",
    "conn.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_s3_objects(aws_s3_bucket_name, aws_s3_object)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
