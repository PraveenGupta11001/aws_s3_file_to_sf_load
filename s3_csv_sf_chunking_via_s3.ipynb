{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3\n",
    "import io\n",
    "import csv\n",
    "from snowflake.snowpark import Session\n",
    "import datetime\n",
    "\n",
    "connection_parameters = {\n",
    "   \"account\": \"dr31778.ca-central-1.aws\",\n",
    "   \"user\": \"PRAVEEN11001\",\n",
    "   \"password\": \"XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX\",\n",
    "   \"role\": \"ACCOUNTADMIN\",  # optional\n",
    "   \"warehouse\": \"COMPUTE_WH\",  # optional\n",
    "   \"database\": \"DEMO\",  # optional\n",
    "   \"schema\": \"s3\"  # optional\n",
    "}\n",
    "\n",
    "session = Session.builder.configs(connection_parameters).create()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "aws_access_key = 'XXXXXXXXXXXXXXXX'\n",
    "aws_secret_key = 'XXXXXXXXXXXXXXXXXXXXXXXXXXXXXXXX'\n",
    "aws_s3_bucket_name = 'praveeng-s3'  \n",
    "aws_s3_object = 'chunked_csv/'\n",
    "\n",
    "s3 = boto3.client('s3', aws_access_key_id=aws_access_key, aws_secret_access_key=aws_secret_key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Uploaded partition 1 of xmls/student-dataset.csv to S3 with key: chunked_csv/student_dataset_0.csv\n",
      "Uploaded partition 2 of xmls/student-dataset.csv to S3 with key: chunked_csv/student_dataset_1.csv\n",
      "Uploaded partition 3 of xmls/student-dataset.csv to S3 with key: chunked_csv/student_dataset_2.csv\n"
     ]
    }
   ],
   "source": [
    "response = s3.list_objects_v2(Bucket=aws_s3_bucket_name, Prefix='xmls')\n",
    "\n",
    "for obj in response['Contents']:\n",
    "    key = obj['Key']\n",
    "    if key.endswith('set.csv'):\n",
    "        obj_size = obj['Size']\n",
    "        response = s3.get_object(Bucket=aws_s3_bucket_name, Key=key)\n",
    "        file_content = response['Body'].read().decode('utf-8')\n",
    "\n",
    "        if obj_size > 10 * 1024:\n",
    "            chunk_size = 10 * 1024  \n",
    "            reader = csv.reader(io.StringIO(file_content))\n",
    "            headers = next(reader)  \n",
    "            chunks = []\n",
    "            current_chunk = [','.join(headers)]  # Include header in the first chunk\n",
    "            current_chunk_size = len(','.join(headers).encode('utf-8'))\n",
    "\n",
    "            for row in reader:\n",
    "                row_str = ','.join(row)\n",
    "                row_size = len(row_str.encode('utf-8'))\n",
    "                if current_chunk_size + row_size <= chunk_size:\n",
    "                    current_chunk.append(row_str)\n",
    "                    current_chunk_size += row_size\n",
    "                else:\n",
    "                    chunks.append(current_chunk)\n",
    "                    current_chunk = [','.join(headers), row_str]  \n",
    "                    current_chunk_size = len(','.join(headers).encode('utf-8')) + row_size\n",
    "\n",
    "            if current_chunk:\n",
    "                chunks.append(current_chunk)\n",
    "\n",
    "            for i, chunk in enumerate(chunks):\n",
    "                partition_key = f\"{aws_s3_object}{key.split('/')[1].split('.')[0]}_{i}.csv\".replace('-', '_')\n",
    "                partition_data = '\\n'.join(chunk)\n",
    "                s3.put_object(Bucket=aws_s3_bucket_name, Key=partition_key, Body=partition_data.encode('utf-8'))\n",
    "                print(f\"Uploaded partition {i + 1} of {key} to S3 with key: {partition_key}\")\n",
    "        else:\n",
    "            s3.put_object(Bucket=aws_s3_bucket_name, Key=f\"{aws_s3_object}{key.split('/')[1]}\".replace('-', '_'), Body=file_content.encode('utf-8'))\n",
    "            print(f\"Uploaded {aws_s3_object}{key.split('/')[1].replace('-', '_')} to S3.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def delete_s3_objects(bucket_name, prefix):\n",
    "    try:\n",
    "        response = s3.list_objects_v2(Bucket=bucket_name, Prefix=prefix)\n",
    "        if 'Contents' in response:\n",
    "            for obj in response['Contents']:\n",
    "                s3.delete_object(Bucket=bucket_name, Key=obj['Key'])\n",
    "                print(f\"Deleted object: {obj['Key']}\")\n",
    "        else:\n",
    "            print(f\"No objects found with prefix: {prefix}\")\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred: {str(e)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "def copy_into_sf(session, folder_name, filename):\n",
    "\n",
    "    copy_date = datetime.datetime.utcnow()\n",
    "    \n",
    "    copy_result = session.sql(f\"\"\"\n",
    "    COPY INTO AWS_STUDENT_DATASET_S3(filename, ID, NAME, NATIONALITY, CITY, \n",
    "                              LATITUDE, LONGITUDE, GENDER, ethinic_group, \n",
    "                              AGE, ENGLISH_GRADE, MATH_GRADE, SCIENCE_GRADE, \n",
    "                              LANGUAGE_GRADE, PORTFOLIO_RATING, COVERLETTER_RATING, \n",
    "                              REFLETTER_RATING, copy_date)\n",
    "    FROM \n",
    "        (SELECT '{filename}' AS filename, \n",
    "            $1 AS ID, \n",
    "            $2 AS NAME, \n",
    "            $3 AS NATIONALITY, \n",
    "            $4 AS CITY, \n",
    "            $5 AS LATITUDE, \n",
    "            $6 AS LONGITUDE,\n",
    "            $7 AS GENDER, \n",
    "            $8 AS ethinic_group, \n",
    "            $9 AS AGE, \n",
    "            $10 AS ENGLISH_GRADE, \n",
    "            $11 AS MATH_GRADE, \n",
    "            $12 AS SCIENCE_GRADE, \n",
    "            $13 AS LANGUAGE_GRADE, \n",
    "            $14 AS PORTFOLIO_RATING, \n",
    "            $15 AS COVERLETTER_RATING, \n",
    "            $16 AS REFLETTER_RATING,\n",
    "            '{copy_date}' AS copy_date \n",
    "        FROM @s3_stage/{folder_name}{filename}.csv)\n",
    "        \n",
    "    ON_ERROR='SKIP_FILE'\n",
    "    FILE_FORMAT= (FORMAT_NAME=CSV_FORMAT);\n",
    "    \"\"\").collect()\n",
    "\n",
    "    print(copy_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Row(file='s3://praveeng-s3/chunked_csv/student_dataset_0.csv', status='LOADED', rows_parsed=118, rows_loaded=118, error_limit=1, errors_seen=0, first_error=None, first_error_line=None, first_error_character=None, first_error_column_name=None)]\n",
      "[Row(file='s3://praveeng-s3/chunked_csv/student_dataset_1.csv', status='LOADED', rows_parsed=116, rows_loaded=116, error_limit=1, errors_seen=0, first_error=None, first_error_line=None, first_error_character=None, first_error_column_name=None)]\n",
      "[Row(file='s3://praveeng-s3/chunked_csv/student_dataset_2.csv', status='LOADED', rows_parsed=73, rows_loaded=73, error_limit=1, errors_seen=0, first_error=None, first_error_line=None, first_error_character=None, first_error_column_name=None)]\n"
     ]
    }
   ],
   "source": [
    "# for all formatted csv file \n",
    "response = s3.list_objects_v2(Bucket=aws_s3_bucket_name, Prefix=aws_s3_object)\n",
    "\n",
    "for obj in response['Contents']:\n",
    "    key = obj['Key']\n",
    "    if key.endswith('.csv'):\n",
    "        filename = key.split('/')[-1].split('.')[0]  \n",
    "        copy_into_sf(session, aws_s3_object, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Deleted object: chunked_csv/student_dataset_0.csv\n",
      "Deleted object: chunked_csv/student_dataset_1.csv\n",
      "Deleted object: chunked_csv/student_dataset_2.csv\n"
     ]
    }
   ],
   "source": [
    "delete_s3_objects(aws_s3_bucket_name, aws_s3_object)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
